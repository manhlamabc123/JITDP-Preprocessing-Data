{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data-set's commits and its test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  3980\n",
      "Number of test examples:  19\n"
     ]
    }
   ],
   "source": [
    "project_name = 'moment'\n",
    "\n",
    "data = pickle.load(open(f'data/{project_name}/{project_name}.pkl', 'rb'))\n",
    "test = pickle.load(open(f'data/test-set-bugged/{project_name}.pkl', 'rb'))\n",
    "\n",
    "print('Number of examples: ', len(data))\n",
    "print('Number of test examples: ', len(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data-set's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  14\n",
      "Features:  ['ns', 'nd', 'nf', 'entrophy', 'la', 'ld', 'lt', 'fix', 'ndev', 'age', 'nuc', 'exp', 'rexp', 'sexp']\n",
      "Number of examples:  2482\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file = f'data/{project_name}/{project_name}_{project_name}_2023-05-01.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_features = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the DataFrame\n",
    "column_names = df_features.columns.tolist()\n",
    "features = column_names[3:]\n",
    "num_rows = df_features.shape[0]\n",
    "print('Number of features: ', len(features))\n",
    "print('Features: ', features)\n",
    "print('Number of examples: ', num_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dictionary mapping commit_hash and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_result = [(item['commit_hash'], item['bug_inducing']) for item in data]\n",
    "result_dict = dict(commit_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of commits for train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All commit:  3980\n",
      "Test commit:  19\n",
      "Features id:  2482\n"
     ]
    }
   ],
   "source": [
    "data_commit_hash_list = [item['commit_hash'] for item in data]\n",
    "test_commit_hash_list = [item['commit_hash'] for item in test]\n",
    "features_id_list = df_features['_id'].tolist()\n",
    "print('All commit: ', len(data_commit_hash_list))\n",
    "print('Test commit: ', len(test_commit_hash_list))\n",
    "print('Features id: ', len(features_id_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common elements in two lists, used to extract test-set\n",
    "def find_common_elements(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    common_elements = list(set1.intersection(set2))\n",
    "    return common_elements\n",
    "\n",
    "# Find elements in list A but not in list B, used to extract train-set\n",
    "def find_unique_elements(list_a, list_b):\n",
    "    unique_elements = [elem for elem in list_a if elem not in list_b]\n",
    "    return unique_elements\n",
    "\n",
    "# Save data using pickle to a directory with given filename\n",
    "def save_list_to_file(directory, filename, data):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)  # Create the directory if it doesn't exist\n",
    "\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    \n",
    "    print(f'Saved the list to file: {filepath}')\n",
    "\n",
    "# Create list of commits for the data-set using a given list of commit_hash\n",
    "def filter_elements_by_commit_hash(commit_hash_list, elements_list):\n",
    "    filtered_elements = [element for element in elements_list if element['commit_hash'] in commit_hash_list]\n",
    "    return filtered_elements\n",
    "\n",
    "# Function to map '_id' values to 'bug' values\n",
    "def map_bug_value(row):\n",
    "    commit_hash = row['_id']\n",
    "    if commit_hash in result_dict:\n",
    "        return result_dict[commit_hash]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Print out info of the data-set\n",
    "def data_info_extract(data):\n",
    "    number_clean = 0\n",
    "    number_defect = 0\n",
    "    for data in data:\n",
    "        if data['bug_inducing'] == 0:\n",
    "            number_clean += 1\n",
    "        else:\n",
    "            number_defect += 1\n",
    "    return number_clean, number_defect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train/test-set and total data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2422 2408 14\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = find_common_elements(data_commit_hash_list, features_id_list)\n",
    "cleaned_test = find_common_elements(test_commit_hash_list, features_id_list)\n",
    "cleaned_train = find_unique_elements(cleaned_data, cleaned_test)\n",
    "print(len(cleaned_data), len(cleaned_train), len(cleaned_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Split the train and test set (test / total = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "Modified Train: 2180\n",
      "Modified Test: 242\n"
     ]
    }
   ],
   "source": [
    "# Example lists\n",
    "list_a = cleaned_train\n",
    "list_b = cleaned_test\n",
    "\n",
    "# Calculate the target length for list b\n",
    "target_length = int(0.1 * (len(list_a) + len(list_b)))\n",
    "print(target_length)\n",
    "\n",
    "# Cut the last element from list a and add it to list b until the target length is reached\n",
    "while len(list_b) < target_length:\n",
    "    last_element = list_a.pop()\n",
    "    list_b.append(last_element)\n",
    "\n",
    "print(\"Modified Train:\", len(cleaned_train))\n",
    "print(\"Modified Test:\", len(cleaned_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of commits for train and test set. And save as .pkl/.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2180 242\n",
      "2180 242\n"
     ]
    }
   ],
   "source": [
    "train = filter_elements_by_commit_hash(cleaned_train, data)\n",
    "test = filter_elements_by_commit_hash(cleaned_test, data)\n",
    "df_features['bug'] = df_features.apply(lambda row: map_bug_value(row), axis=1)\n",
    "train_features = df_features[df_features['_id'].isin(cleaned_train)]\n",
    "test_features = df_features[df_features['_id'].isin(cleaned_test)]\n",
    "print(len(train), len(test))\n",
    "print(train_features.shape[0], test_features.shape[0])\n",
    "\n",
    "save_list_to_file(f'data/{project_name}/clean', f'{project_name}_train.pkl', train)\n",
    "save_list_to_file(f'data/{project_name}/clean', f'{project_name}_test.pkl', test)\n",
    "train_features.to_csv(f'data/{project_name}/clean/{project_name}_feature_train.csv', index=False)\n",
    "test_features.to_csv(f'data/{project_name}/clean/{project_name}_feature_test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-set commits information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncleaned Data  | Total: 3980 - Clean: 3005 - Defect: 975 - Rate: 0.2449748743718593\n",
      "Cleaned Train   | Total: 2180 - Clean: 1464 - Defect: 716 - Rate: 0.3284403669724771\n",
      "Cleaned Test    | Total: 242 - Clean: 163 - Defect: 79 - Rate: 0.32644628099173556\n"
     ]
    }
   ],
   "source": [
    "number_clean, number_defect = data_info_extract(data)\n",
    "print(f\"Uncleaned Data  | Total: {number_clean + number_defect} - Clean: {number_clean} - Defect: {number_defect} - Rate: {number_defect / (number_clean + number_defect)}\")\n",
    "number_clean, number_defect = data_info_extract(train)\n",
    "print(f\"Cleaned Train   | Total: {number_clean + number_defect} - Clean: {number_clean} - Defect: {number_defect} - Rate: {number_defect / (number_clean + number_defect)}\")\n",
    "number_clean, number_defect = data_info_extract(test)\n",
    "print(f\"Cleaned Test    | Total: {number_clean + number_defect} - Clean: {number_clean} - Defect: {number_defect} - Rate: {number_defect / (number_clean + number_defect)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-set features information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train   | Total: 2180 - Clean: 1464 - Defect: 716 - Rate: 0.3284403669724771\n",
      "Cleaned Train   | Total: 242 - Clean: 163 - Defect: 79 - Rate: 0.32644628099173556\n"
     ]
    }
   ],
   "source": [
    "number_clean = train_features['bug'].value_counts().get(0, 0)\n",
    "number_defect = train_features['bug'].value_counts().get(1, 0)\n",
    "print(f\"Cleaned Train   | Total: {number_clean + number_defect} - Clean: {number_clean} - Defect: {number_defect} - Rate: {number_defect / (number_clean + number_defect)}\")\n",
    "\n",
    "number_clean = test_features['bug'].value_counts().get(0, 0)\n",
    "number_defect = test_features['bug'].value_counts().get(1, 0)\n",
    "print(f\"Cleaned Train   | Total: {number_clean + number_defect} - Clean: {number_clean} - Defect: {number_defect} - Rate: {number_defect / (number_clean + number_defect)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manh-simcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
